{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment4_Leo .ipynb","version":"0.3.2","provenance":[{"file_id":"160UFjK4qcJ77V9dBiUlihfEyfd6oippA","timestamp":1554388091763}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"ZCyiDg8WztlF","colab_type":"code","outputId":"a0247e09-21d6-4f7c-8763-f66263dd99fa","executionInfo":{"status":"ok","timestamp":1555347990860,"user_tz":240,"elapsed":22068,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive/')\n","os.chdir('/content/drive/My Drive/DS8009 Deep Learning/Week 10')\n","os.listdir()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['turTest2KForStudent.txt',\n"," 'en1k.txt',\n"," 'turTrain6K.txt',\n"," 'fra9k.txt',\n"," 'week10.pptx',\n"," 'turTrain6K.gdoc',\n"," 'turTest2KForStudent.gdoc',\n"," '8009 Week10 lab .ipynb',\n"," 's2s.h5',\n"," 'week10.gslides',\n"," 'Copy of 8009 Week10 lab .ipynb',\n"," 'result_assign4Leo.txt',\n"," 'result_assign4Leo.gdoc',\n"," 'result_assign4_bidirectionalLeo.txt',\n"," 'turTest2K.txt',\n"," 'Assignment4_Leo .ipynb']"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"uq9XvGKi9Q5a","colab_type":"text"},"cell_type":"markdown","source":["https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n","\n","**References**\n","- [Sequence to Sequence Learning with Neural Networks\n","   ](https://arxiv.org/abs/1409.3215)\n","- [Learning Phrase Representations using\n","    RNN Encoder-Decoder for Statistical Machine Translation\n","    ](https://arxiv.org/abs/1406.1078)\n","    \n","    \n","'''\n","\n","Data set\n","\n","\n","http://www.manythings.org/anki/"]},{"metadata":{"id":"aLcka37A9zY-","colab_type":"text"},"cell_type":"markdown","source":["# Assignment 4 Begins"]},{"metadata":{"id":"1aAiZ3Nw3wnC","colab_type":"code","outputId":"65526f52-2ecd-4227-d568-5b4b63ee7371","executionInfo":{"status":"ok","timestamp":1554999653532,"user_tz":240,"elapsed":1064,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"cell_type":"code","source":["from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense, GRU, Embedding, Bidirectional,Input,Concatenate\n","import numpy as np\n","\n","batch_size = 64  # Batch size for training.\n","epochs = 5  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = 'turTrain6K.txt'\n","\n","# Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text = line.split('\\t')\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])\n","\n","print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)\n","\n","input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 6000\n","Number of unique input tokens: 30\n","Number of unique output tokens: 31\n","Max sequence length for inputs: 19\n","Max sequence length for outputs: 16\n"],"name":"stdout"}]},{"metadata":{"id":"ljeTs_0uz2xF","colab_type":"code","outputId":"34782afb-ffd1-471c-9dc1-a407228a20d2","executionInfo":{"status":"ok","timestamp":1555003217957,"user_tz":240,"elapsed":2229,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["#reading my test data\n","test_text = []\n","test_char = set()\n","#setting up our test data into a list\n","with open('turTest2KForStudent.txt', 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    #test_text = line.split('\\t')\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    test_text.append(line)\n","    for line in test_text:\n","      for char in line:\n","        if char not in test_char:\n","          test_char.add(char)\n","          \n","test_char = sorted(list(test_char))\n","num_encoder2_tokens = len(test_char)\n","max_encoder2_seq_length = max([len(txt) for txt in test_text])\n","\n","print('Number of samples:', len(test_text))\n","print('Number of unique input tokens:', num_encoder2_tokens)\n","print('Max sequence length for inputs:', max_encoder2_seq_length)\n","\n","test_token_index = dict(\n","    [(char, i) for i, char in enumerate(test_char)])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of samples: 2000\n","Number of unique input tokens: 30\n","Max sequence length for inputs: 18\n"],"name":"stdout"}]},{"metadata":{"id":"lezw_Ud0cyUl","colab_type":"code","colab":{}},"cell_type":"code","source":["encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vkqID3CX3shn","colab_type":"code","outputId":"dc5602a4-108e-4c93-84e9-747a62aae46c","executionInfo":{"status":"ok","timestamp":1554997921901,"user_tz":240,"elapsed":25758,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["my_text = 'masallar'\n","placeholder = np.zeros((1,len(my_text)+10,num_encoder_tokens))\n","\n","for i, char in enumerate(my_text):\n","    print(i,char, input_token_index[char])\n","    placeholder[0,i,input_token_index[char]] = 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 m 12\n","1 a 0\n","2 s 17\n","3 a 0\n","4 l 11\n","5 l 11\n","6 a 0\n","7 r 16\n"],"name":"stdout"}]},{"metadata":{"id":"JWjqsERYmQkQ","colab_type":"code","colab":{}},"cell_type":"code","source":["#encoding testing data\n","\n","encoder_test_text = np.zeros(\n","    (len(test_text), max_encoder2_seq_length, num_encoder2_tokens),\n","    dtype='float32')\n","\n","for i, (test_text) in enumerate(test_text):\n","    for t, char in enumerate(test_text):\n","        encoder_test_text[i, t, test_token_index[char]] = 1."],"execution_count":0,"outputs":[]},{"metadata":{"id":"2zc-4rY1mlyl","colab_type":"code","colab":{}},"cell_type":"code","source":["#basic LSTM encoder/decoder\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XKo6JkhKK1Jb","colab_type":"code","colab":{}},"cell_type":"code","source":["#Define an input sequence and process it bidirectional LSTM\n","\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = Bidirectional(LSTM(latent_dim, return_state=True))                      \n","encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n","state_h = Concatenate()([forward_h, backward_h])\n","state_c = Concatenate()([forward_c, backward_c])\n","encoder_states = [state_h, state_c]\n","\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qkV1Ks-7l_jY","colab_type":"code","outputId":"711abfee-c272-4926-947e-fc24a1c5555d","executionInfo":{"status":"ok","timestamp":1555003185632,"user_tz":240,"elapsed":3243330,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":3451}},"cell_type":"code","source":["model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='RMSprop', loss='categorical_crossentropy')\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=128,\n","          epochs=100,\n","          validation_split=0.1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 5400 samples, validate on 600 samples\n","Epoch 1/100\n","5400/5400 [==============================] - 37s 7ms/step - loss: 1.0400 - val_loss: 1.0322\n","Epoch 2/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.8943 - val_loss: 0.8630\n","Epoch 3/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.7544 - val_loss: 0.6617\n","Epoch 4/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.6115 - val_loss: 0.5844\n","Epoch 5/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.4936 - val_loss: 0.4829\n","Epoch 6/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.3932 - val_loss: 0.3578\n","Epoch 7/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.3096 - val_loss: 0.3037\n","Epoch 8/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.2419 - val_loss: 0.2608\n","Epoch 9/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.1901 - val_loss: 0.2566\n","Epoch 10/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.1499 - val_loss: 0.2248\n","Epoch 11/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.1224 - val_loss: 0.1311\n","Epoch 12/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0966 - val_loss: 0.1743\n","Epoch 13/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0772 - val_loss: 0.1588\n","Epoch 14/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0664 - val_loss: 0.1058\n","Epoch 15/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0535 - val_loss: 0.0919\n","Epoch 16/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0451 - val_loss: 0.0771\n","Epoch 17/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0363 - val_loss: 0.0789\n","Epoch 18/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0306 - val_loss: 0.1074\n","Epoch 19/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0290 - val_loss: 0.0636\n","Epoch 20/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0236 - val_loss: 0.0969\n","Epoch 21/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0206 - val_loss: 0.1038\n","Epoch 22/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0180 - val_loss: 0.0820\n","Epoch 23/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0156 - val_loss: 0.0837\n","Epoch 24/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0148 - val_loss: 0.0769\n","Epoch 25/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0145 - val_loss: 0.0574\n","Epoch 26/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0110 - val_loss: 0.0717\n","Epoch 27/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0102 - val_loss: 0.0720\n","Epoch 28/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0105 - val_loss: 0.0698\n","Epoch 29/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0087 - val_loss: 0.1061\n","Epoch 30/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0083 - val_loss: 0.1047\n","Epoch 31/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0094 - val_loss: 0.0690\n","Epoch 32/100\n","5400/5400 [==============================] - 31s 6ms/step - loss: 0.0079 - val_loss: 0.0577\n","Epoch 33/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0068 - val_loss: 0.0620\n","Epoch 34/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0053 - val_loss: 0.0989\n","Epoch 35/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0069 - val_loss: 0.1033\n","Epoch 36/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0080 - val_loss: 0.0568\n","Epoch 37/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0046 - val_loss: 0.0682\n","Epoch 38/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0051 - val_loss: 0.0669\n","Epoch 39/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0052 - val_loss: 0.0573\n","Epoch 40/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0043 - val_loss: 0.0590\n","Epoch 41/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0045 - val_loss: 0.0679\n","Epoch 42/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0038 - val_loss: 0.0628\n","Epoch 43/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0041 - val_loss: 0.0782\n","Epoch 44/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0040 - val_loss: 0.0654\n","Epoch 45/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0034 - val_loss: 0.0722\n","Epoch 46/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0041 - val_loss: 0.0685\n","Epoch 47/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0045 - val_loss: 0.0665\n","Epoch 48/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0035 - val_loss: 0.0646\n","Epoch 49/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0034 - val_loss: 0.0878\n","Epoch 50/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0049 - val_loss: 0.0685\n","Epoch 51/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0035 - val_loss: 0.0667\n","Epoch 52/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0029 - val_loss: 0.0700\n","Epoch 53/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0030 - val_loss: 0.0673\n","Epoch 54/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0032 - val_loss: 0.0658\n","Epoch 55/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0039 - val_loss: 0.0622\n","Epoch 56/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0024 - val_loss: 0.0649\n","Epoch 57/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0032 - val_loss: 0.0724\n","Epoch 58/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0034 - val_loss: 0.0632\n","Epoch 59/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0025 - val_loss: 0.0668\n","Epoch 60/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0021 - val_loss: 0.0741\n","Epoch 61/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0027 - val_loss: 0.0749\n","Epoch 62/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0030 - val_loss: 0.0720\n","Epoch 63/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0019 - val_loss: 0.0572\n","Epoch 64/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0027 - val_loss: 0.0760\n","Epoch 65/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0024 - val_loss: 0.0710\n","Epoch 66/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0018 - val_loss: 0.0665\n","Epoch 67/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0018 - val_loss: 0.0622\n","Epoch 68/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0021 - val_loss: 0.0852\n","Epoch 69/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0023 - val_loss: 0.0757\n","Epoch 70/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0027 - val_loss: 0.0721\n","Epoch 71/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0017 - val_loss: 0.0998\n","Epoch 72/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0021 - val_loss: 0.0878\n","Epoch 73/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0019 - val_loss: 0.0698\n","Epoch 74/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0020 - val_loss: 0.0697\n","Epoch 75/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0016 - val_loss: 0.0674\n","Epoch 76/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0027 - val_loss: 0.0678\n","Epoch 77/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0019 - val_loss: 0.0717\n","Epoch 78/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0016 - val_loss: 0.0741\n","Epoch 79/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0022 - val_loss: 0.0812\n","Epoch 80/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0015 - val_loss: 0.0742\n","Epoch 81/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0023 - val_loss: 0.0895\n","Epoch 82/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0016 - val_loss: 0.0939\n","Epoch 83/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0017 - val_loss: 0.0890\n","Epoch 84/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0020 - val_loss: 0.0682\n","Epoch 85/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0022 - val_loss: 0.0777\n","Epoch 86/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0012 - val_loss: 0.0667\n","Epoch 87/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0021 - val_loss: 0.0692\n","Epoch 88/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0012 - val_loss: 0.0701\n","Epoch 89/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0018 - val_loss: 0.0726\n","Epoch 90/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0019 - val_loss: 0.0763\n","Epoch 91/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0020 - val_loss: 0.0712\n","Epoch 92/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0013 - val_loss: 0.0749\n","Epoch 93/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0018 - val_loss: 0.0801\n","Epoch 94/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0013 - val_loss: 0.0691\n","Epoch 95/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0017 - val_loss: 0.0753\n","Epoch 96/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0021 - val_loss: 0.0735\n","Epoch 97/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0015 - val_loss: 0.0688\n","Epoch 98/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 9.7448e-04 - val_loss: 0.0752\n","Epoch 99/100\n","5400/5400 [==============================] - 32s 6ms/step - loss: 0.0019 - val_loss: 0.0813\n","Epoch 100/100\n","5400/5400 [==============================] - 33s 6ms/step - loss: 0.0012 - val_loss: 0.0733\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f65a70af160>"]},"metadata":{"tags":[]},"execution_count":41}]},{"metadata":{"id":"2X-VGJxjnAnx","colab_type":"code","colab":{}},"cell_type":"code","source":["#LSTM Encoder\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BQVP0ZmjOUHy","colab_type":"code","colab":{}},"cell_type":"code","source":["#Bidirectional LSTM Encoder\n","encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim*2,))\n","decoder_state_input_c = Input(shape=(latent_dim*2,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)\n","\n","# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wWptF79LnJvX","colab_type":"code","colab":{}},"cell_type":"code","source":["def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S26icW3Q7e9K","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_index =1\n","input_texts[seq_index]\n","encoder_input_data[seq_index: seq_index + 1].shape\n","#encoder_input_dat\n","#decode_sequence()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CjbayYNcnMcX","colab_type":"code","outputId":"53d0b3be-f4f6-42dc-f809-86b32e325338","executionInfo":{"status":"ok","timestamp":1555003197733,"user_tz":240,"elapsed":1774,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"cell_type":"code","source":["for seq_index in range(10):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-\n","Input sentence: şarabın\n","Decoded sentence: şarap\n","\n","-\n","Input sentence: ret\n","Decoded sentence: red\n","\n","-\n","Input sentence: masallar\n","Decoded sentence: masal\n","\n","-\n","Input sentence: konserlere\n","Decoded sentence: konser\n","\n","-\n","Input sentence: armağanı\n","Decoded sentence: armağan\n","\n","-\n","Input sentence: sıkıyordu\n","Decoded sentence: sık\n","\n","-\n","Input sentence: bulguların\n","Decoded sentence: bulgu\n","\n","-\n","Input sentence: yarına\n","Decoded sentence: yarın\n","\n","-\n","Input sentence: sandıklar\n","Decoded sentence: sandık\n","\n","-\n","Input sentence: modelini\n","Decoded sentence: model\n","\n"],"name":"stdout"}]},{"metadata":{"id":"ExmtvosFN4g4","colab_type":"code","outputId":"59adcfcf-0950-49ce-e75e-40f66b87e62e","executionInfo":{"status":"ok","timestamp":1555003223297,"user_tz":240,"elapsed":410,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":697}},"cell_type":"code","source":["for seq_index in range(10):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_test_text[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', test_text[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["-\n","Input sentence: bıyıkları\n","Decoded sentence: bıyık\n","\n","-\n","Input sentence: ayrımında\n","Decoded sentence: ayrım\n","\n","-\n","Input sentence: hazırlığı\n","Decoded sentence: hazırlık\n","\n","-\n","Input sentence: seviyorum\n","Decoded sentence: sev\n","\n","-\n","Input sentence: genişlikte\n","Decoded sentence: geniş\n","\n","-\n","Input sentence: hazırlayın\n","Decoded sentence: hazırla\n","\n","-\n","Input sentence: olmam\n","Decoded sentence: ol\n","\n","-\n","Input sentence: tavrıyla\n","Decoded sentence: tavrı\n","\n","-\n","Input sentence: düşünmüyorsun\n","Decoded sentence: düşün\n","\n","-\n","Input sentence: belgelere\n","Decoded sentence: belge\n","\n"],"name":"stdout"}]},{"metadata":{"id":"pxRMuyoPoAHu","colab_type":"code","colab":{}},"cell_type":"code","source":["result = []\n","for seq_index in range(len(test_text)):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_test_text[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    result.append(decoded_sentence)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9LvfRYGqP5hJ","colab_type":"code","colab":{}},"cell_type":"code","source":["#writing the results onto a .txt file\n","with open('result_assign4_bidirectionalLeo.txt', 'w') as f:\n","    for item in result:\n","        f.write(\"%s\" % item)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PTpuAHqNcLB2","colab_type":"text"},"cell_type":"markdown","source":["**Insert Accuracy**"]},{"metadata":{"id":"ZfY8xM3aaScl","colab_type":"code","colab":{}},"cell_type":"code","source":["result2 = list(zip(test_text,result))\n","result2[0][1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ePIetiV7am5U","colab_type":"code","outputId":"d5f45c63-ebf1-4dc9-f18e-881004b89e72","executionInfo":{"status":"error","timestamp":1555348111305,"user_tz":240,"elapsed":476,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":232}},"cell_type":"code","source":["clean_target = []\n","\n","for i in range(len(target_texts)):\n","  clean = target_texts[i].strip()\n","  clean_target.append(clean)\n","\n","clean_target"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-a616a6b823c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mclean_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'target_texts' is not defined"]}]},{"metadata":{"id":"oK-Om1eudQQO","colab_type":"code","outputId":"bb49eb80-702a-4318-9f2c-03735db36132","executionInfo":{"status":"ok","timestamp":1555003276215,"user_tz":240,"elapsed":511,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":17034}},"cell_type":"code","source":["clean_result = []\n","\n","for i in range(len(result)):\n","  clean = result[i].strip()\n","  clean_result.append(clean)\n","\n","clean_result"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['bıyık',\n"," 'ayrım',\n"," 'hazırlık',\n"," 'sev',\n"," 'geniş',\n"," 'hazırla',\n"," 'ol',\n"," 'tavrı',\n"," 'düşün',\n"," 'belge',\n"," 'yavrum',\n"," 'bina',\n"," 'öneri',\n"," 'can',\n"," 'sevgili',\n"," 'taban',\n"," 'ay',\n"," 'okul',\n"," 'biz',\n"," 'bağla',\n"," 'akıl',\n"," 'adale',\n"," 'zaman',\n"," 'kapı',\n"," 'aile',\n"," 'zıkım',\n"," 'dur',\n"," 'ilgilen',\n"," 'baş',\n"," 'silaf',\n"," 'ense',\n"," 'ver',\n"," 'aydınlat',\n"," 'değişim',\n"," 'heyben',\n"," 'kafa',\n"," 'karı',\n"," 'anal',\n"," 'baş',\n"," 'düş',\n"," 'gerek',\n"," 'dil',\n"," 'damla',\n"," 'yön',\n"," 'anla',\n"," 'yarat',\n"," 'milletekçil',\n"," 'ara',\n"," 'kal',\n"," 'amara',\n"," 'hava',\n"," 'bakan',\n"," 'kum',\n"," 'dal',\n"," 'ziyaret',\n"," 'cins',\n"," 'tarih',\n"," 'hepsi',\n"," 'şem',\n"," 'iç',\n"," 'git',\n"," 'azal',\n"," 'aşk',\n"," 'getir',\n"," 'yet',\n"," 'ağız',\n"," 'ver',\n"," 'performans',\n"," 'dizi',\n"," 'kitap',\n"," 'tezgat',\n"," 'alet',\n"," 'bulgu',\n"," 'hal',\n"," 'sahahlık',\n"," 'öğe',\n"," 'zih',\n"," 'buyur',\n"," 'mağaza',\n"," 'cumarbaş',\n"," 'tut',\n"," 'gel',\n"," 'getir',\n"," 'mücadele',\n"," 'baba',\n"," 'damar',\n"," 'kullanım',\n"," 'akıl',\n"," 'hayret',\n"," 'kanal',\n"," 'buyur',\n"," 'ol',\n"," 'alışveriş',\n"," 'dur',\n"," 'hayat',\n"," 'hazırla',\n"," 'miktar',\n"," 'koku',\n"," 'diz',\n"," 'kupon',\n"," 'kulak',\n"," 'uyuş',\n"," 'saç',\n"," 'kamyon',\n"," 'platform',\n"," 'gece',\n"," 'sevince',\n"," 'gözlem',\n"," 'bırak',\n"," 'yan',\n"," 'parla',\n"," 'yık',\n"," 'gece',\n"," 'kavga',\n"," 'etki',\n"," 'anlat',\n"," 'kababat',\n"," 'madde',\n"," 'radyo',\n"," 'meyhane',\n"," 'alıştır',\n"," 'vaai',\n"," 'kereenele',\n"," 'özet',\n"," 'teker',\n"," 'geliş',\n"," 'bul',\n"," 'üzer',\n"," 'kravat',\n"," 'anla',\n"," 'kızak',\n"," 'mandera',\n"," 'parmak',\n"," 'defya',\n"," 'kuşku',\n"," 'çık',\n"," 'uçak',\n"," 'öneri',\n"," 'öngörü',\n"," 'işim',\n"," 'taban',\n"," 'düzey',\n"," 'metin',\n"," 'ortam',\n"," 'küpe',\n"," 'bulun',\n"," 'kalk',\n"," 'delikanlı',\n"," 'hayat',\n"," 'hal',\n"," 'öl',\n"," 'müşteri',\n"," 'sev',\n"," 'yaşam',\n"," 'hazırla',\n"," 'yemek',\n"," 'çizgi',\n"," 'pantolon',\n"," 'maat',\n"," 'boşluk',\n"," 'haber',\n"," 'anne',\n"," 'siz',\n"," 'bölge',\n"," 'çalış',\n"," 'kanıt',\n"," 'iste',\n"," 'sevgili',\n"," 'istiham',\n"," 'açılla',\n"," 'ağla',\n"," 'tel',\n"," 'inek',\n"," 'yemek',\n"," 'yük',\n"," 'arka',\n"," 'avukat',\n"," 'yat',\n"," 'hırsa',\n"," 'ser',\n"," 'çal',\n"," 'viski',\n"," 'mal',\n"," 'boy',\n"," 'öde',\n"," 'iskele',\n"," 'hipotez',\n"," 'neşe',\n"," 'kural',\n"," 'dönüşüm',\n"," 'bul',\n"," 'güneş',\n"," 'yakın',\n"," 'köylü',\n"," 'ver',\n"," 'baş',\n"," 'kaydet',\n"," 'kana',\n"," 'önlük',\n"," 'koy',\n"," 'karanlık',\n"," 'cinayet',\n"," 'gör',\n"," 'ürk',\n"," 'köpek',\n"," 'çalış',\n"," 'metin',\n"," 'gel',\n"," 'öğle',\n"," 'teyze',\n"," 'hayat',\n"," 'doğa',\n"," 'köpek',\n"," 'rol',\n"," 'ihtimam',\n"," 'merak',\n"," 'alem',\n"," 'defa',\n"," 'gir',\n"," 'güç',\n"," 'nen',\n"," 'darbuka',\n"," 'kır',\n"," 'homuransa',\n"," 'iste',\n"," 'oyna',\n"," 'hizmet',\n"," 'taşım',\n"," 'yut',\n"," 'arad',\n"," 'hatırla',\n"," 'geç',\n"," 'uyuşturucu',\n"," 'geçmiş',\n"," 'fırla',\n"," 'kavaalıt',\n"," 'memunyayet',\n"," 'şirket',\n"," 'at',\n"," 'niyet',\n"," 'de',\n"," 'sistem',\n"," 'müvekkile',\n"," 'kay',\n"," 'gör',\n"," 'su',\n"," 'evlen',\n"," 'aç',\n"," 'televizyon',\n"," 'kız',\n"," 'kavram',\n"," 'sus',\n"," 'deneş',\n"," 'sayı',\n"," 'ülke',\n"," 'küney',\n"," 'sıra',\n"," 'şaş',\n"," 'değil',\n"," 'sil',\n"," 'enflasyon',\n"," 'koru',\n"," 'ada',\n"," 'tartış',\n"," 'kalem',\n"," 'bura',\n"," 'gürültü',\n"," 'yemek',\n"," 'yardım',\n"," 'kur',\n"," 'boy',\n"," 'atal',\n"," 'hatırla',\n"," 'kural',\n"," 'isim',\n"," 'bah',\n"," 'kar',\n"," 'şirket',\n"," 'dizalgo',\n"," 'ne',\n"," 'kendi',\n"," 'cumarbaş',\n"," 'sor',\n"," 'pazar',\n"," 'yer',\n"," 'ver',\n"," 'erik',\n"," 'kendi',\n"," 'başla',\n"," 'tavan',\n"," 'amaç',\n"," 'şey',\n"," 'kuram',\n"," 'git',\n"," 'yabancı',\n"," 'zih',\n"," 'ev',\n"," 'kazak',\n"," 'kardeş',\n"," 'yolculuk',\n"," 'hak',\n"," 'mı',\n"," 'türetim',\n"," 'maşitet',\n"," 'mu',\n"," 'köy',\n"," 'ders',\n"," 'biriknetik',\n"," 'bura',\n"," 'an',\n"," 'uçak',\n"," 'sor',\n"," 'soba',\n"," 'kasıt',\n"," 'tarih',\n"," 'yat',\n"," 'el',\n"," 'ilke',\n"," 'masa',\n"," 'bardak',\n"," 'başrol',\n"," 'ön',\n"," 'rus',\n"," 'dalga',\n"," 'el',\n"," 'yasak',\n"," 'dosya',\n"," 'başkomutan',\n"," 'kent',\n"," 'düşünce',\n"," 'sözleşme',\n"," 'kulak',\n"," 'kade',\n"," 'gör',\n"," 'hepsi',\n"," 'bul',\n"," 'çinli',\n"," 'bileşim',\n"," 'cezave',\n"," 'delik',\n"," 'bık',\n"," 'ihale',\n"," 'dinam',\n"," 'bişop',\n"," 'kıyı',\n"," 'numara',\n"," 'mücadele',\n"," 'öl',\n"," 'sağol',\n"," 'dil',\n"," 'yele',\n"," 'şekfat',\n"," 'al',\n"," 'yer',\n"," 'bir',\n"," 'çek',\n"," 'yumurta',\n"," 'iç',\n"," 'kart',\n"," 'evre',\n"," 'aile',\n"," 'surgula',\n"," 'sabır',\n"," 'eroin',\n"," 'otur',\n"," 'gündem',\n"," 'düş',\n"," 'mırıldan',\n"," 'kanal',\n"," 'fırın',\n"," 'farkı',\n"," 'fıkra',\n"," 'seçim',\n"," 'yaşam',\n"," 'plan',\n"," 'karşı',\n"," 'müdürlük',\n"," 'yol',\n"," 'kardeş',\n"," 'ırmak',\n"," 'ton',\n"," 'kesim',\n"," 'vur',\n"," 'ara',\n"," 'zeyt',\n"," 'boğ',\n"," 'uğraş',\n"," 'boya',\n"," 'örtüş',\n"," 'it',\n"," 'çarpıntı',\n"," 'sorun',\n"," 'erkek',\n"," 'bit',\n"," 'soğan',\n"," 'sistem',\n"," 'iddia',\n"," 'kağıt',\n"," 'faiz',\n"," 'ökke',\n"," 'mağaza',\n"," 'oran',\n"," 'ışık',\n"," 'politika',\n"," 'geç',\n"," 'bey',\n"," 'rastla',\n"," 'tas',\n"," 'taraf',\n"," 'sıkıntı',\n"," 'vakıf',\n"," 'bekle',\n"," 'topla',\n"," 'kitap',\n"," 'sıra',\n"," 'iste',\n"," 'yer',\n"," 'defter',\n"," 'mağaza',\n"," 'şaşır',\n"," 'kamyon',\n"," 'delik',\n"," 'yap',\n"," 'ol',\n"," 'zorla',\n"," 'sataş',\n"," 'arka',\n"," 'kop',\n"," 'kaavan',\n"," 'anlat',\n"," 'gel',\n"," 'kafa',\n"," 'emir',\n"," 'mont',\n"," 'ölü',\n"," 'kolarcık',\n"," 'us',\n"," 'seçim',\n"," 'kadın',\n"," 'öğüt',\n"," 'kork',\n"," 'ver',\n"," 'kitap',\n"," 'sil',\n"," 'art',\n"," 'karın',\n"," 'teknoloji',\n"," 'rapor',\n"," 'kemir',\n"," 'duvar',\n"," 'mumam',\n"," 'oda',\n"," 'acı',\n"," 'demokrasi',\n"," 'istem',\n"," 'kod',\n"," 'sit',\n"," 'muhallebi',\n"," 'hükettm',\n"," 'soru',\n"," 'tarz',\n"," 'toplantı',\n"," 'sonuç',\n"," 'sar',\n"," 'kok',\n"," 'yap',\n"," 'ulaş',\n"," 'bar',\n"," 'parla',\n"," 'yan',\n"," 'otur',\n"," 'koşul',\n"," 'otur',\n"," 'harca',\n"," 'alıntı',\n"," 'korku',\n"," 'sıra',\n"," 'bekle',\n"," 'gramenyan',\n"," 'firma',\n"," 'yer',\n"," 'organ',\n"," 'uza',\n"," 'sez',\n"," 'kampan',\n"," 'doktor',\n"," 'uy',\n"," 'ilgi',\n"," 'tessi',\n"," 'türün',\n"," 'hazırla',\n"," 'salla',\n"," 'pürtük',\n"," 'yetki',\n"," 'benze',\n"," 'süre',\n"," 'adam',\n"," 'kahve',\n"," 'ol',\n"," 'ödeme',\n"," 'sigara',\n"," 'nakil',\n"," 'havza',\n"," 'tiksin',\n"," 'kardeş',\n"," 'rahatla',\n"," 'tren',\n"," 'açıkla',\n"," 'kuruluş',\n"," 'gözlem',\n"," 'yaşama',\n"," 'değin',\n"," 'masa',\n"," 'yürek',\n"," 'sak',\n"," 'firma',\n"," 'ilgilen',\n"," 'et',\n"," 'dinle',\n"," 'anla',\n"," 'aşı',\n"," 'bak',\n"," 'keşfet',\n"," 'iniş',\n"," 'viçdan',\n"," 'gelir',\n"," 'okaş',\n"," 'kabuk',\n"," 'yabancı',\n"," 'gir',\n"," 'kork',\n"," 'kadın',\n"," 'toprak',\n"," 'inan',\n"," 'savun',\n"," 'gel',\n"," 'anlat',\n"," 'heyet',\n"," 'kalk',\n"," 'yapsa',\n"," 'koy',\n"," 'üniversite',\n"," 'hayat',\n"," 'köy',\n"," 'sektör',\n"," 'ara',\n"," 'başlangıç',\n"," 'et',\n"," 'son',\n"," 'hayvan',\n"," 'yaşa',\n"," 'ver',\n"," 'kal',\n"," 'gelecek',\n"," 'yamur',\n"," 'ulaş',\n"," 'şey',\n"," 'hük',\n"," 'aile',\n"," 'ölçü',\n"," 'hayat',\n"," 'kabuk',\n"," 'cirol',\n"," 'gök',\n"," 'balık',\n"," 'ağız',\n"," 'ol',\n"," 'söyle',\n"," 'özellik',\n"," 'din',\n"," 'kahve',\n"," 'şerek',\n"," 'yanaş',\n"," 'rekor',\n"," 'gezinti',\n"," 'gün',\n"," 'uyku',\n"," 'tirador',\n"," 'ceza',\n"," 'yapı',\n"," 'hayat',\n"," 'çık',\n"," 'pencere',\n"," 'üzüntü',\n"," 'tarih',\n"," 'gör',\n"," 'hak',\n"," 'türk',\n"," 'yer',\n"," 'masa',\n"," 'sonra',\n"," 'anlam',\n"," 'nefes',\n"," 'muhabir',\n"," 'konuş',\n"," 'pantolon',\n"," 'ün',\n"," 'kader',\n"," 'akıl',\n"," 'işletmeci',\n"," 'kavga',\n"," 'dal',\n"," 'kayıt',\n"," 'bil',\n"," 'ardım',\n"," 'uçak',\n"," 'beyin',\n"," 'uyku',\n"," 'ortaklık',\n"," 'yürü',\n"," 'ruh',\n"," 'san',\n"," 'utan',\n"," 'olan',\n"," 'düşün',\n"," 'çekin',\n"," 'bekle',\n"," 'bütçe',\n"," 'betim',\n"," 'kişilik',\n"," 'göğüs',\n"," 'cum',\n"," 'temizlo',\n"," 'babala',\n"," 'iklim',\n"," 'yap',\n"," 'ol',\n"," 'birlik',\n"," 'teori',\n"," 'san',\n"," 'bakım',\n"," 'tabak',\n"," 'söz',\n"," 'yön',\n"," 'ayrıntı',\n"," 'başşanın',\n"," 'al',\n"," 'barınak',\n"," 'destek',\n"," 'karpi',\n"," 'vavget',\n"," 'yazı',\n"," 'cetvel',\n"," 'tavır',\n"," 'esa',\n"," 'kova',\n"," 'yüz',\n"," 'buraber',\n"," 'pati',\n"," 'arkadaş',\n"," 'cerrah',\n"," 'yaşam',\n"," 'eleman',\n"," 'öncelik',\n"," 'et',\n"," 'kavim',\n"," 'halt',\n"," 'orsouu',\n"," 'taşı',\n"," 'dönem',\n"," 'eleştiri',\n"," 'gürültü',\n"," 'bul',\n"," 'salon',\n"," 'duş',\n"," 'karış',\n"," 'kendi',\n"," 'koridor',\n"," 'çevir',\n"," 'rota',\n"," 'not',\n"," 'pay',\n"," 'deri',\n"," 'köşe',\n"," 'çek',\n"," 'dert',\n"," 'aşk',\n"," 'kavram',\n"," 'sevgili',\n"," 'tepe',\n"," 'kitap',\n"," 'yan',\n"," 'çık',\n"," 'cumaret',\n"," 'sinir',\n"," 'tanı',\n"," 'konu',\n"," 'fikir',\n"," 'ayır',\n"," 'kasa',\n"," 'tütsü',\n"," 'bina',\n"," 'kalabaç',\n"," 'yemek',\n"," 'art',\n"," 'durum',\n"," 'el',\n"," 'büyü',\n"," 'yanıt',\n"," 'su',\n"," 'telaş',\n"," 'makine',\n"," 'kıvrım',\n"," 'ses',\n"," 'yiyecek',\n"," 'otur',\n"," 'yaka',\n"," 'pangra',\n"," 'kurt',\n"," 'ışılda',\n"," 'cenaze',\n"," 'sıra',\n"," 'yürek',\n"," 'gerilim',\n"," 'peş',\n"," 'konuş',\n"," 'uygulasa',\n"," 'darıl',\n"," 'dönüş',\n"," 'araç',\n"," 'gerek',\n"," 'uçak',\n"," 'tayga',\n"," 'parmak',\n"," 'neden',\n"," 'yanıt',\n"," 'yıl',\n"," 'baba',\n"," 'petroli',\n"," 'tanı',\n"," 'kıvınlık',\n"," 'kıskan',\n"," 'cam',\n"," 'tabak',\n"," 'sanır',\n"," 'dava',\n"," 'tür',\n"," 'saat',\n"," 'kaybet',\n"," 'nokta',\n"," 'pilot',\n"," 'yer',\n"," 'öner',\n"," 'baş',\n"," 'sev',\n"," 'kız',\n"," 'turunca',\n"," 'yarı',\n"," 'zanne',\n"," 'gel',\n"," 'görüşme',\n"," 'hal',\n"," 'gece',\n"," 'lokanta',\n"," 'yers',\n"," 'misafir',\n"," 'baş',\n"," 'olay',\n"," 'kan',\n"," 'siyaset',\n"," 'toplum',\n"," 'köylü',\n"," 'iste',\n"," 'bak',\n"," 'tekli',\n"," 'köy',\n"," 'izin',\n"," 'fassu',\n"," 'genç',\n"," 'ön',\n"," 'dil',\n"," 'kaset',\n"," 'dayı',\n"," 'yabancı',\n"," 'siçim',\n"," 'yaşa',\n"," 'hava',\n"," 'konuş',\n"," 'kuruntu',\n"," 'gel',\n"," 'gazete',\n"," 'kalem',\n"," 'ölü',\n"," 'git',\n"," 'gazete',\n"," 'kov',\n"," 'varlık',\n"," 'vurgula',\n"," 'ver',\n"," 'yakın',\n"," 'kitap',\n"," 'parti',\n"," 'dalga',\n"," 'mantık',\n"," 'boynuz',\n"," 'durum',\n"," 'turşu',\n"," 'pilot',\n"," 'kesint',\n"," 'kes',\n"," 'kasık',\n"," 'kal',\n"," 'güç',\n"," 'sonra',\n"," 'flaster',\n"," 'soru',\n"," 'çene',\n"," 'kafa',\n"," 'duyar',\n"," 'komutan',\n"," 'tak',\n"," 'resim',\n"," 'göz',\n"," 'araclık',\n"," 'ay',\n"," 'yarar',\n"," 'aygıt',\n"," 'perde',\n"," 'temerrüt',\n"," 'sevin',\n"," 'peş',\n"," 'gir',\n"," 'en',\n"," 'sürün',\n"," 'hata',\n"," 'hak',\n"," 'lokanta',\n"," 'tasarım',\n"," 'uyarı',\n"," 'karşıl',\n"," 'gıta',\n"," 'saat',\n"," 'famli',\n"," 'durum',\n"," 'gün',\n"," 'teknoloji',\n"," 'okul',\n"," 'bilgi',\n"," 'içtenik',\n"," 'anla',\n"," 'ses',\n"," 'hepsi',\n"," 'parçacık',\n"," 'tasarı',\n"," 'katıl',\n"," 'pilot',\n"," 'miktar',\n"," 'parti',\n"," 'uçak',\n"," 'amca',\n"," 'tepe',\n"," 'boşluk',\n"," 'bale',\n"," 'suç',\n"," 'kürs',\n"," 'hatırla',\n"," 'harçlık',\n"," 'gurur',\n"," 'torlum',\n"," 'anımsa',\n"," 'topla',\n"," 'adam',\n"," 'ihracat',\n"," 'güç',\n"," 'sayfa',\n"," 'konu',\n"," 'çiz',\n"," 'duy',\n"," 'kita',\n"," 'geyik',\n"," 'karşı',\n"," 'gör',\n"," 'komaptan',\n"," 'taşın',\n"," 'başarı',\n"," 'sakınca',\n"," 'fezleke',\n"," 'davet',\n"," 'bit',\n"," 'nohut',\n"," 'mağara',\n"," 'üretim',\n"," 'uçak',\n"," 'vurul',\n"," 'ol',\n"," 'mahmet',\n"," 'alın',\n"," 'boyun',\n"," 'aşım',\n"," 'cinayet',\n"," 'kullan',\n"," 'tahkikam',\n"," 'illenemen',\n"," 'faaliyet',\n"," 'tasarı',\n"," 'siz',\n"," 'en',\n"," 'alan',\n"," 'koca',\n"," 'öbür',\n"," 'dur',\n"," 'ağız',\n"," 'kahraman',\n"," 'ara',\n"," 'pilot',\n"," 'sıraa',\n"," 'saç',\n"," 'fırla',\n"," 'başla',\n"," 'hedef',\n"," 'nasipa',\n"," 'aday',\n"," 'gazete',\n"," 'düşün',\n"," 'gızdkbak',\n"," 'tazminat',\n"," 'mu',\n"," 'et',\n"," 'hemşire',\n"," 'tutsak',\n"," 'üretim',\n"," 'gir',\n"," 'öp',\n"," 'hiçbiri',\n"," 'yüz',\n"," 'poli',\n"," 'yaz',\n"," 'teorem',\n"," 'ölçü',\n"," 'yuka',\n"," 'göster',\n"," 'köpek',\n"," 'çevre',\n"," 'üzer',\n"," 'inanç',\n"," 'et',\n"," 'duy',\n"," 'hare',\n"," 'ortam',\n"," 'hal',\n"," 'balık',\n"," 'izin',\n"," 'kadav',\n"," 'karşı',\n"," 'enişte',\n"," 'surat',\n"," 'kıyı',\n"," 'oku',\n"," 'tramvay',\n"," 'neden',\n"," 'rozet',\n"," 'geçmiş',\n"," 'lokanta',\n"," 'bilgi',\n"," 'kurtar',\n"," 'ara',\n"," 'duman',\n"," 'at',\n"," 'layık',\n"," 'kavram',\n"," 'süzül',\n"," 'kafa',\n"," 'not',\n"," 'tahah',\n"," 'bak',\n"," 'eroin',\n"," 'ney',\n"," 'protesta',\n"," 'iç',\n"," 'çevre',\n"," 'dert',\n"," 'oku',\n"," 'gir',\n"," 'facia',\n"," 'vade',\n"," 'çatı',\n"," 'kapamak',\n"," 'gez',\n"," 'temsilci',\n"," 'gövde',\n"," 'laf',\n"," 'otur',\n"," 'arı',\n"," 'ver',\n"," 'rekabet',\n"," 'planla',\n"," 'yanıl',\n"," 'vücut',\n"," 'etki',\n"," 'önem',\n"," 'bulun',\n"," 'tanık',\n"," 'atılım',\n"," 'çevir',\n"," 'temel',\n"," 'bak',\n"," 'hayat',\n"," 'adet',\n"," 'vakit',\n"," 'dön',\n"," ...]"]},"metadata":{"tags":[]},"execution_count":50}]},{"metadata":{"id":"rmpB2qEwqFJg","colab_type":"code","outputId":"e236b4b6-86fd-4351-fffe-ff7ccd177154","executionInfo":{"status":"ok","timestamp":1555007491975,"user_tz":240,"elapsed":544,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["counter = 0 \n","for word in clean_result:\n","  if word in clean_target:\n","    counter +=1\n","    \n","counter"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1609"]},"metadata":{"tags":[]},"execution_count":63}]},{"metadata":{"id":"VerltAmwsIQZ","colab_type":"code","colab":{}},"cell_type":"code","source":["with open('result_assign4Leo.txt', 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","    \n","lines"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V1mcT4A1sTo8","colab_type":"code","outputId":"60f8126b-179e-4f67-e11d-3e446d0241d0","executionInfo":{"status":"ok","timestamp":1555007898833,"user_tz":240,"elapsed":768,"user":{"displayName":"Leo Tsang","photoUrl":"","userId":"10778462888507238936"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["counter = 0 \n","for word in lines:\n","  if word in clean_target:\n","    counter +=1\n","    \n","counter"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1564"]},"metadata":{"tags":[]},"execution_count":65}]},{"metadata":{"id":"OgI2jOSx-JZ9","colab_type":"code","colab":{}},"cell_type":"code","source":["result_assign4_bidirectionalLeo.txt"],"execution_count":0,"outputs":[]}]}